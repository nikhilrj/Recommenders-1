{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n\n<i>Licensed under the MIT License.</i>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# LightGBM: A Highly Efficient Gradient Boosting Decision Tree\nThis notebook will give you an example of how to train a LightGBM model to estimate click-through rates on an e-commerce advertisement. We will train a LightGBM based model on a [publicly available dataset from Criteo](https://www.kaggle.com/c/criteo-display-ad-challenge).\n\n[LightGBM](https://github.com/Microsoft/LightGBM) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n* Fast training speed and high efficiency.\n* Low memory usage.\n* Great accuracy.\n* Support for parallel and GPU learning.\n* Capable of handling large-scale data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Global Settings and Imports"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Install these packages for this notebook in Azure Notebooks\n!pip install papermill==0.19.1 category_encoders>=1.3.0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import sys, os\nsys.path.append(\"../../\")\nimport numpy as np\nimport lightgbm as lgb\nimport papermill as pm\nimport pandas as pd\nimport category_encoders as ce\nfrom tempfile import TemporaryDirectory\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nimport reco_utils.recommender.lightgbm.lightgbm_utils as lgb_utils\nimport reco_utils.dataset.criteo as criteo\n\nprint(\"System version: {}\".format(sys.version))\nprint(\"LightGBM version: {}\".format(lgb.__version__))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "System version: 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n[GCC 7.2.0]\nLightGBM version: 2.2.1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Parameter Setting\nLet's set the main related parameters for LightGBM now. Basically, the task is a binary classification (predicting click or no click), so the objective function is set to binary logloss, and 'AUC' metric, is used as a metric which is less effected by imbalance in the classes of the dataset.\n\nGenerally, we can adjust the number of leaves (MAX_LEAF), the minimum number of data in each leaf (MIN_DATA), maximum number of trees (NUM_OF_TREES), the learning rate of trees (TREE_LEARNING_RATE) and EARLY_STOPPING_ROUNDS (to avoid overfitting) in the model to get better performance.\n\nBesides, we can also adjust some other listed parameters to optimize the results. [In this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst), a list of all the parameters is shown. Also, some advice on how to tune these parameters can be found [in this url](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-Tuning.rst). "
    },
    {
      "metadata": {
        "tags": [
          "parameters"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "MAX_LEAF = 64\nMIN_DATA = 20\nNUM_OF_TREES = 100\nTREE_LEARNING_RATE = 0.15\nEARLY_STOPPING_ROUNDS = 20\nMETRIC = \"auc\"\nSIZE = \"sample\"",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'num_class': 1,\n    'objective': \"binary\",\n    'metric': METRIC,\n    'num_leaves': MAX_LEAF,\n    'min_data': MIN_DATA,\n    'boost_from_average': True,\n    #set it according to your cpu cores.\n    'num_threads': 20,\n    'feature_fraction': 0.8,\n    'learning_rate': TREE_LEARNING_RATE,\n}",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Preparation\nHere we use CSV format as the example data input. Our example data is a sample (about 100 thousand samples) from [Criteo that was used in a previous kaggle competition](https://www.kaggle.com/c/criteo-display-ad-challenge). The Criteo dataset is a well-known industry benchmarking dataset for developing CTR prediction models, and it's frequently adopted as an evaluation dataset by research papers. The original dataset is too large for a lightweight demo, so we sample a small portion from it as a demo dataset.\n\nSpecifically, there are 39 columns of features in Criteo, where 13 columns are numerical features (I1-I13) and the other 26 columns are categorical features (C1-C26)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nume_cols = [\"I\" + str(i) for i in range(1, 14)]\ncate_cols = [\"C\" + str(i) for i in range(1, 27)]\nlabel_col = \"Label\"\n\nheader = [label_col] + nume_cols + cate_cols\nwith TemporaryDirectory() as tmp:\n    all_data = criteo.load_pandas_df(size=SIZE, local_cache_path=tmp, header=header)\ndisplay(all_data.head())",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "8.79MB [00:04, 1.77MB/s]                            \n",
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Label</th>\n      <th>I1</th>\n      <th>I2</th>\n      <th>I3</th>\n      <th>I4</th>\n      <th>I5</th>\n      <th>I6</th>\n      <th>I7</th>\n      <th>I8</th>\n      <th>I9</th>\n      <th>...</th>\n      <th>C17</th>\n      <th>C18</th>\n      <th>C19</th>\n      <th>C20</th>\n      <th>C21</th>\n      <th>C22</th>\n      <th>C23</th>\n      <th>C24</th>\n      <th>C25</th>\n      <th>C26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>1382.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>2.0</td>\n      <td>181.0</td>\n      <td>...</td>\n      <td>e5ba7672</td>\n      <td>f54016b9</td>\n      <td>21ddcdc9</td>\n      <td>b1252a9d</td>\n      <td>07b5194c</td>\n      <td>NaN</td>\n      <td>3a171ecb</td>\n      <td>c5c50484</td>\n      <td>e8b83407</td>\n      <td>9727dd16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>44.0</td>\n      <td>1.0</td>\n      <td>102.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>07c540c4</td>\n      <td>b04e4670</td>\n      <td>21ddcdc9</td>\n      <td>5840adea</td>\n      <td>60f6221e</td>\n      <td>NaN</td>\n      <td>3a171ecb</td>\n      <td>43f13e8b</td>\n      <td>e8b83407</td>\n      <td>731c3655</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>767.0</td>\n      <td>89.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>245.0</td>\n      <td>...</td>\n      <td>8efede7f</td>\n      <td>3412118d</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>e587c466</td>\n      <td>ad3062eb</td>\n      <td>3a171ecb</td>\n      <td>3b183c5c</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>893</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4392.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1e88c74f</td>\n      <td>74ef3502</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6b3a5ca6</td>\n      <td>NaN</td>\n      <td>3a171ecb</td>\n      <td>9117a34a</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3.0</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1e88c74f</td>\n      <td>26b3c7a7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>21c9516a</td>\n      <td>NaN</td>\n      <td>32c7478e</td>\n      <td>b34f3128</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 40 columns</p>\n</div>",
            "text/plain": "   Label   I1   I2    I3    I4      I5    I6    I7   I8     I9    ...     \\\n0      0  1.0    1   5.0   0.0  1382.0   4.0  15.0  2.0  181.0    ...      \n1      0  2.0    0  44.0   1.0   102.0   8.0   2.0  2.0    4.0    ...      \n2      0  2.0    0   1.0  14.0   767.0  89.0   4.0  2.0  245.0    ...      \n3      0  NaN  893   NaN   NaN  4392.0   NaN   0.0  0.0    0.0    ...      \n4      0  3.0   -1   NaN   0.0     2.0   0.0   3.0  0.0    0.0    ...      \n\n        C17       C18       C19       C20       C21       C22       C23  \\\n0  e5ba7672  f54016b9  21ddcdc9  b1252a9d  07b5194c       NaN  3a171ecb   \n1  07c540c4  b04e4670  21ddcdc9  5840adea  60f6221e       NaN  3a171ecb   \n2  8efede7f  3412118d       NaN       NaN  e587c466  ad3062eb  3a171ecb   \n3  1e88c74f  74ef3502       NaN       NaN  6b3a5ca6       NaN  3a171ecb   \n4  1e88c74f  26b3c7a7       NaN       NaN  21c9516a       NaN  32c7478e   \n\n        C24       C25       C26  \n0  c5c50484  e8b83407  9727dd16  \n1  43f13e8b  e8b83407  731c3655  \n2  3b183c5c       NaN       NaN  \n3  9117a34a       NaN       NaN  \n4  b34f3128       NaN       NaN  \n\n[5 rows x 40 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, we create three datasets that we will use throughout estimation and evaluation:\n\n- `train_data` (first 80%): used to estimate the model\n- `valid_data` (middle 10%): used to validate during training\n- `test_data` (last 10%): used to validate after training\n\nNote that the dataset is a time-series, which is also very common in recommendation scenario, so we split perform a chronological split."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# split data to 3 sets    \nlength = len(all_data)\ntrain_data = all_data.loc[:0.8*length-1]\nvalid_data = all_data.loc[0.8*length:0.9*length-1]\ntest_data = all_data.loc[0.9*length:]",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Basic Usage\n\n### Ordinal Encoding\n\nLightGBM can handle low-frequency features and missing value, so for basic usage, we only encode the string-like categorical features by an ordinal encoder. We use the standard [ordinal encoder](http://contrib.scikit-learn.org/categorical-encoding/ordinal.html) from the `category_encoders` module."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n\ndef encode_csv(df, encoder, label_col, typ='fit'):\n    if typ == 'fit':\n        df = encoder.fit_transform(df)\n    else:\n        df = encoder.transform(df)\n    y = df[label_col].values\n    del df[label_col]\n    return df, y\n\ntrain_x, train_y = encode_csv(train_data, ord_encoder, label_col)\nvalid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, 'transform')\ntest_x, test_y = encode_csv(test_data, ord_encoder, label_col, 'transform')\n\nprint('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n      .format(trn_x_shape=train_x.shape,\n              trn_y_shape=train_y.shape,\n              vld_x_shape=valid_x.shape,\n              vld_y_shape=valid_y.shape,\n              tst_x_shape=test_x.shape,\n              tst_y_shape=test_y.shape,))\ntrain_x.head()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train Data Shape: X: (80000, 39); Y: (80000,).\nValid Data Shape: X: (10000, 39); Y: (10000,).\nTest Data Shape: X: (10000, 39); Y: (10000,).\n\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>I1</th>\n      <th>I2</th>\n      <th>I3</th>\n      <th>I4</th>\n      <th>I5</th>\n      <th>I6</th>\n      <th>I7</th>\n      <th>I8</th>\n      <th>I9</th>\n      <th>I10</th>\n      <th>...</th>\n      <th>C17</th>\n      <th>C18</th>\n      <th>C19</th>\n      <th>C20</th>\n      <th>C21</th>\n      <th>C22</th>\n      <th>C23</th>\n      <th>C24</th>\n      <th>C25</th>\n      <th>C26</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>1382.0</td>\n      <td>4.0</td>\n      <td>15.0</td>\n      <td>2.0</td>\n      <td>181.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>0</td>\n      <td>44.0</td>\n      <td>1.0</td>\n      <td>102.0</td>\n      <td>8.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>14.0</td>\n      <td>767.0</td>\n      <td>89.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>245.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>893</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4392.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.0</td>\n      <td>-1</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 39 columns</p>\n</div>",
            "text/plain": "    I1   I2    I3    I4      I5    I6    I7   I8     I9  I10 ...   C17  C18  \\\n0  1.0    1   5.0   0.0  1382.0   4.0  15.0  2.0  181.0  1.0 ...     1    1   \n1  2.0    0  44.0   1.0   102.0   8.0   2.0  2.0    4.0  1.0 ...     2    2   \n2  2.0    0   1.0  14.0   767.0  89.0   4.0  2.0  245.0  1.0 ...     3    3   \n3  NaN  893   NaN   NaN  4392.0   NaN   0.0  0.0    0.0  NaN ...     4    4   \n4  3.0   -1   NaN   0.0     2.0   0.0   3.0  0.0    0.0  1.0 ...     4    5   \n\n   C19  C20  C21  C22  C23  C24  C25  C26  \n0    1    1    1    1    1    1    1    1  \n1    1    2    2    1    1    2    1    2  \n2    2    3    3    2    1    3    2    3  \n3    2    3    4    1    1    4    2    3  \n4    2    3    5    1    2    5    2    3  \n\n[5 rows x 39 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create model\nWhen both hyper-parameters and data are ready, we can create a model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\nlgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\nlgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\nlgb_model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round=NUM_OF_TREES,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      valid_sets=lgb_valid,\n                      categorical_feature=cate_cols)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[1]\tvalid_0's auc: 0.728695\nTraining until validation scores don't improve for 20 rounds.\n[2]\tvalid_0's auc: 0.742373\n[3]\tvalid_0's auc: 0.747298\n[4]\tvalid_0's auc: 0.747969\n[5]\tvalid_0's auc: 0.751102\n[6]\tvalid_0's auc: 0.753734\n[7]\tvalid_0's auc: 0.755335\n[8]\tvalid_0's auc: 0.75658\n[9]\tvalid_0's auc: 0.757071\n[10]\tvalid_0's auc: 0.758572\n[11]\tvalid_0's auc: 0.759742\n[12]\tvalid_0's auc: 0.760415\n[13]\tvalid_0's auc: 0.760602\n[14]\tvalid_0's auc: 0.761192\n[15]\tvalid_0's auc: 0.7616\n[16]\tvalid_0's auc: 0.761697\n[17]\tvalid_0's auc: 0.762255\n[18]\tvalid_0's auc: 0.76253\n[19]\tvalid_0's auc: 0.763092\n[20]\tvalid_0's auc: 0.762172\n[21]\tvalid_0's auc: 0.762066\n[22]\tvalid_0's auc: 0.761866\n[23]\tvalid_0's auc: 0.761433\n[24]\tvalid_0's auc: 0.761588\n[25]\tvalid_0's auc: 0.761017\n[26]\tvalid_0's auc: 0.761086\n[27]\tvalid_0's auc: 0.761177\n[28]\tvalid_0's auc: 0.760893\n[29]\tvalid_0's auc: 0.760635\n[30]\tvalid_0's auc: 0.760104\n[31]\tvalid_0's auc: 0.759298\n[32]\tvalid_0's auc: 0.759176\n[33]\tvalid_0's auc: 0.758384\n[34]\tvalid_0's auc: 0.758168\n[35]\tvalid_0's auc: 0.757902\n[36]\tvalid_0's auc: 0.758005\n[37]\tvalid_0's auc: 0.757782\n[38]\tvalid_0's auc: 0.757542\n[39]\tvalid_0's auc: 0.756966\nEarly stopping, best iteration is:\n[19]\tvalid_0's auc: 0.763092\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's see what is the model's performance:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_preds = lgb_model.predict(test_x)\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\nres_basic = {\"auc\": auc, \"logloss\": logloss}\nprint(res_basic)\npm.record(\"res_basic\", res_basic)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'auc': 0.7674356153037237, 'logloss': 0.466876775528735}\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.1). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "res_basic": {
                "logloss": 0.466876775528735,
                "auc": 0.76743561530372373
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n## Optimized Usage\n\n### Label-encoding and Binary-encoding\n\nNext, we will iterate on this model to see if we can make it better.\n\nSpecifically we will do some feature engineering to treat the categorical variables differently. We will convert all the categorical features in the original dataset into numeric features by label-encoding [3] and binary-encoding [4]. \n\nDue to the time-series nature of the Criteo dataset, the label-encoding we adopted is executed one-by-one, which means we encode the samples (i.e. rows) in order, and we incorporate the information from the previous samples into the features for the current row (sequential label-encoding and sequential count-encoding). We also do a few additional clean up tasks. See [lgb_utils.NumEncoder](https://github.com/microsoft/recommenders/blob/master/reco_utils/recommender/lightgbm/lightgbm_utils.py) for details.\n\nSpecifically, in `lgb_utils.NumEncoder`, the main steps are as follows:\n\n* First, we convert the low-frequency categorical features to `\"LESS\"` and the missing categorical features to `\"UNK\"`. \n* Second, we convert the missing numerical features into the mean of corresponding columns. \n* Third, the string-like categorical features are ordinal encoded like the example above. \n* Fourth, we label-encode the categorical features in the samples order one-by-one. For each sample, we add information about the label and count of its previous samples to produce new features. For each categorical variable, we create a new label-encoded feature ($LF$) such that for the current sample $x_i$, we calculate the average click through rate in all previous samples ($j=1..(i-1)$) where the category value was the same $c$. Formally, this looks like: \n$$LF = \\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c) \\cdot y}{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}$$\nwhere $x_i$ is the $i$th sample, $c$ is the observed category for $x_i$, and $I(\\cdot)$ is the indicator function that determines whether a *former* sample contains $c$ or not.\n* Fifth, we also add the count frequency of $c$ as a new count feature ($CF$). This formally evaluates to:\n$$CF = \\frac{\\sum\\nolimits_{j=1}^{i-1} I(x_j=c)}{i-1}$$ \n* Finally, based on the results of ordinal encoding, we add the binary encoding results as new columns into the data.\n\nNote that the statistics used in the above process only updates when fitting the training set, while maintaining static when transforming the testing set because the label of test data should be considered as unknown."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "label_col = 'Label'\nnum_encoder = lgb_utils.NumEncoder(cate_cols, nume_cols, label_col)\ntrain_x, train_y = num_encoder.fit_transform(train_data)\nvalid_x, valid_y = num_encoder.transform(valid_data)\ntest_x, test_y = num_encoder.transform(test_data)\ndel num_encoder\nprint('Train Data Shape: X: {trn_x_shape}; Y: {trn_y_shape}.\\nValid Data Shape: X: {vld_x_shape}; Y: {vld_y_shape}.\\nTest Data Shape: X: {tst_x_shape}; Y: {tst_y_shape}.\\n'\n      .format(trn_x_shape=train_x.shape,\n              trn_y_shape=train_y.shape,\n              vld_x_shape=valid_x.shape,\n              vld_y_shape=valid_y.shape,\n              tst_x_shape=test_x.shape,\n              tst_y_shape=test_y.shape,))\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2019-06-05 13:30:02,641 [INFO] Filtering and fillna features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:08<00:00,  8.32it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 420.94it/s]\n2019-06-05 13:30:11,698 [INFO] Ordinal encoding cate features\n2019-06-05 13:30:14,632 [INFO] Target encoding cate features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:06<00:00,  4.15it/s]\n2019-06-05 13:30:21,066 [INFO] Start manual binary encoding\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:07<00:00,  8.29it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:15<00:00,  1.70it/s]\n2019-06-05 13:30:44,676 [INFO] Filtering and fillna features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 132.71it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 988.76it/s]\n2019-06-05 13:30:44,900 [INFO] Ordinal encoding cate features\n2019-06-05 13:30:44,990 [INFO] Target encoding cate features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 35.08it/s]\n2019-06-05 13:30:45,736 [INFO] Start manual binary encoding\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:04<00:00, 14.53it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:02<00:00, 11.62it/s]\n2019-06-05 13:30:52,601 [INFO] Filtering and fillna features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 127.04it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 1742.10it/s]\n2019-06-05 13:30:52,830 [INFO] Ordinal encoding cate features\n2019-06-05 13:30:52,934 [INFO] Target encoding cate features\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 37.15it/s]\n2019-06-05 13:30:53,644 [INFO] Start manual binary encoding\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:04<00:00, 13.39it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:01<00:00, 13.91it/s]\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Train Data Shape: X: (80000, 268); Y: (80000, 1).\nValid Data Shape: X: (10000, 268); Y: (10000, 1).\nTest Data Shape: X: (10000, 268); Y: (10000, 1).\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training and Evaluation"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params)\nlgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train)\nlgb_model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round=NUM_OF_TREES,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      valid_sets=lgb_valid)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[1]\tvalid_0's auc: 0.731759\nTraining until validation scores don't improve for 20 rounds.\n[2]\tvalid_0's auc: 0.747705\n[3]\tvalid_0's auc: 0.751667\n[4]\tvalid_0's auc: 0.75589\n[5]\tvalid_0's auc: 0.758054\n[6]\tvalid_0's auc: 0.758094\n[7]\tvalid_0's auc: 0.759904\n[8]\tvalid_0's auc: 0.761098\n[9]\tvalid_0's auc: 0.761744\n[10]\tvalid_0's auc: 0.762308\n[11]\tvalid_0's auc: 0.762473\n[12]\tvalid_0's auc: 0.763606\n[13]\tvalid_0's auc: 0.764222\n[14]\tvalid_0's auc: 0.765004\n[15]\tvalid_0's auc: 0.765933\n[16]\tvalid_0's auc: 0.766507\n[17]\tvalid_0's auc: 0.767192\n[18]\tvalid_0's auc: 0.767284\n[19]\tvalid_0's auc: 0.767859\n[20]\tvalid_0's auc: 0.768619\n[21]\tvalid_0's auc: 0.769045\n[22]\tvalid_0's auc: 0.768987\n[23]\tvalid_0's auc: 0.769601\n[24]\tvalid_0's auc: 0.77011\n[25]\tvalid_0's auc: 0.770183\n[26]\tvalid_0's auc: 0.770539\n[27]\tvalid_0's auc: 0.77096\n[28]\tvalid_0's auc: 0.771164\n[29]\tvalid_0's auc: 0.771296\n[30]\tvalid_0's auc: 0.771402\n[31]\tvalid_0's auc: 0.771596\n[32]\tvalid_0's auc: 0.771476\n[33]\tvalid_0's auc: 0.771697\n[34]\tvalid_0's auc: 0.77169\n[35]\tvalid_0's auc: 0.771836\n[36]\tvalid_0's auc: 0.771832\n[37]\tvalid_0's auc: 0.771948\n[38]\tvalid_0's auc: 0.772098\n[39]\tvalid_0's auc: 0.772136\n[40]\tvalid_0's auc: 0.771748\n[41]\tvalid_0's auc: 0.771748\n[42]\tvalid_0's auc: 0.771724\n[43]\tvalid_0's auc: 0.771676\n[44]\tvalid_0's auc: 0.77169\n[45]\tvalid_0's auc: 0.771916\n[46]\tvalid_0's auc: 0.771864\n[47]\tvalid_0's auc: 0.771851\n[48]\tvalid_0's auc: 0.771891\n[49]\tvalid_0's auc: 0.771629\n[50]\tvalid_0's auc: 0.771984\n[51]\tvalid_0's auc: 0.772085\n[52]\tvalid_0's auc: 0.771988\n[53]\tvalid_0's auc: 0.771778\n[54]\tvalid_0's auc: 0.771485\n[55]\tvalid_0's auc: 0.7715\n[56]\tvalid_0's auc: 0.770778\n[57]\tvalid_0's auc: 0.770816\n[58]\tvalid_0's auc: 0.770408\n[59]\tvalid_0's auc: 0.770489\nEarly stopping, best iteration is:\n[39]\tvalid_0's auc: 0.772136\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_preds = lgb_model.predict(test_x)\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\nres_optim = {\"auc\": auc, \"logloss\": logloss}\nprint(res_optim)\npm.record(\"res_optim\", res_optim)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'auc': 0.7757371640011422, 'logloss': 0.4606505068849181}\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Function record is deprecated and will be removed in verison 1.0.0 (current version 0.19.1). Please see `scrapbook.glue` (nteract-scrapbook) as a replacement for this functionality.\n",
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/papermill.record+json": {
              "res_optim": {
                "logloss": 0.4606505068849181,
                "auc": 0.77573716400114223
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Model saving and loading\nNow we finish the basic training and testing for LightGBM, next let's try to save and reload the model, and then evaluate it again."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "with TemporaryDirectory() as tmp:\n    save_file = os.path.join(tmp, r'finished.model')\n    lgb_model.save_model(save_file)\n    loaded_model = lgb.Booster(model_file=save_file)\n\n# eval the performance again\ntest_preds = loaded_model.predict(test_x)\n\nauc = roc_auc_score(np.asarray(test_y.reshape(-1)), np.asarray(test_preds))\nlogloss = log_loss(np.asarray(test_y.reshape(-1)), np.asarray(test_preds), eps=1e-12)\nprint({\"auc\": auc, \"logloss\": logloss})",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'auc': 0.7757371640011422, 'logloss': 0.4606505068849181}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Additional Reading\n\n\\[1\\] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems. 3146â€“3154.<br>\n\\[2\\] The parameters of LightGBM: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst <br>\n\\[3\\] Anna Veronika Dorogush, Vasily Ershov, and Andrey Gulin. 2018. CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363 (2018).<br>\n\\[4\\] Scikit-learn. 2018. categorical_encoding. https://github.com/scikit-learn-contrib/categorical-encoding<br>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}